{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch torchvision\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport torchvision\nfrom torchvision import transforms\n\n# Hyperparameters \nimage_size = 32\npatch_size = 4\nnum_classes = 10\nembed_dim = 384\nnum_heads = 8\nnum_layers = 8\nmlp_ratio = 4\ndropout_rate = 0.1\nbatch_size = 128\nepochs = 100\nlearning_rate = 3e-4\nweight_decay= 0.05\n\n# GPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Data augmentation \ntrain_transforms = transforms.Compose([\n    transforms.RandomCrop(image_size, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1)),\n    transforms.RandomRotation(degrees=15), \n    transforms.ToTensor(),\n\n    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2471, 0.2435, 0.2616])\n])\ntest_transforms = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2471, 0.2435, 0.2616])\n])\n\n# Load CIFAR-10 \ntrain_dataset = torchvision.datasets.CIFAR10(\n    root='data', train=True, download=True, transform=train_transforms)\ntest_dataset = torchvision.datasets.CIFAR10(\n    root='data', train=False, download=True, transform=test_transforms)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\nprint(f\"Train batches: {len(train_loader)}, Test batches: {len(test_loader)}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T14:45:16.440753Z","iopub.execute_input":"2025-10-04T14:45:16.440998Z","iopub.status.idle":"2025-10-04T14:46:54.343191Z","shell.execute_reply.started":"2025-10-04T14:45:16.440970Z","shell.execute_reply":"2025-10-04T14:46:54.342540Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170M/170M [00:14<00:00, 11.9MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Train batches: 391, Test batches: 79\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"class PatchEmbedding(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, embed_dim, dropout):\n        super().__init__()\n        num_patches = (image_size // patch_size) ** 2\n        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n        # CLS token \n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))\n        self.dropout = nn.Dropout(dropout)\n        self.num_patches = num_patches\n\n    def forward(self, x):\n        B = x.shape[0]\n        # Convert \n        x = self.proj(x)\n        # Flatten patches\n        x = x.flatten(2).transpose(1, 2)  # (B, N, embed_dim)\n        # Expand CLS token \n        cls_tokens = self.cls_token.expand(B, -1, -1)  \n        x = torch.cat((cls_tokens, x), dim=1)          \n        x = x + self.pos_embed\n        x = self.dropout(x)\n        return x\n\nclass TransformerEncoderLayer(nn.Module):\n\n    def __init__(self, embed_dim, num_heads, mlp_ratio=4, dropout=0.0):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads)\n        self.norm2 = nn.LayerNorm(embed_dim)\n        # MLP \n        self.mlp = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim * mlp_ratio),\n            nn.GELU(),\n            nn.Linear(embed_dim * mlp_ratio, embed_dim)\n        )\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        res = x\n        x = self.norm1(x)\n        x_t = x.transpose(0, 1)              \n        attn_output, _ = self.attn(x_t, x_t, x_t)  \n        attn_output = attn_output.transpose(0, 1)  \n        x = res + self.dropout(attn_output)\n        # Feed-forward MLP with residual\n        res2 = x\n        x = self.norm2(x)\n        x = self.mlp(x)\n        x = res2 + self.dropout(x)\n        return x\n\nclass VisionTransformer(nn.Module):\n    \"\"\"Vision Transformer (ViT) model for image classification.\"\"\"\n    def __init__(self, image_size=32, patch_size=4, in_channels=3,\n                 num_classes=10, embed_dim=128, num_heads=8,\n                 num_layers=4, mlp_ratio=4, dropout=0.1):\n        super().__init__()\n        # Embedding layer\n        self.embedding = PatchEmbedding(image_size, patch_size, in_channels, embed_dim, dropout)\n        # Transformer encoder layers\n        self.encoder_layers = nn.ModuleList([\n            TransformerEncoderLayer(embed_dim, num_heads, mlp_ratio, dropout)\n            for _ in range(num_layers)\n        ])\n        # Final classification head \n        self.classifier = nn.Linear(embed_dim, num_classes)\n\n    def forward(self, x):\n\n        x = self.embedding(x)  \n        for layer in self.encoder_layers:\n            x = layer(x)\n        # Extract the CLS token \n        cls_token_final = x[:, 0]  # (B, embed_dim)\n        out = self.classifier(cls_token_final)  # (B, num_classes)\n        return out\n\nmodel = VisionTransformer(\n    image_size=image_size, patch_size=patch_size, in_channels=3,\n    num_classes=num_classes, embed_dim=embed_dim,\n    num_heads=num_heads, num_layers=num_layers,\n    mlp_ratio=mlp_ratio, dropout=dropout_rate\n).to(device)\nprint(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T14:46:54.344408Z","iopub.execute_input":"2025-10-04T14:46:54.344722Z","iopub.status.idle":"2025-10-04T14:46:54.609318Z","shell.execute_reply.started":"2025-10-04T14:46:54.344695Z","shell.execute_reply":"2025-10-04T14:46:54.608723Z"}},"outputs":[{"name":"stdout","text":"VisionTransformer(\n  (embedding): PatchEmbedding(\n    (proj): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder_layers): ModuleList(\n    (0-7): 8 x TransformerEncoderLayer(\n      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      (attn): MultiheadAttention(\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n      )\n      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      (mlp): Sequential(\n        (0): Linear(in_features=384, out_features=1536, bias=True)\n        (1): GELU(approximate='none')\n        (2): Linear(in_features=1536, out_features=384, bias=True)\n      )\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (classifier): Linear(in_features=384, out_features=10, bias=True)\n)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\noptimizer = optim.AdamW(model.parameters(), lr=learning_rate,weight_decay=0.05 )\n\n\n#scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs*0.5), int(epochs*0.75)], gamma=0.1)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\nfor epoch in range(1, epochs+1):\n    model.train()\n    total_loss = 0.0\n    total_correct = 0\n    total_samples = 0\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)                   # (B, num_classes)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n      \n        total_loss += loss.item() * images.size(0)\n        preds = outputs.argmax(dim=1)\n        total_correct += (preds == labels).sum().item()\n        total_samples += images.size(0)\n    # Update scheduler\n    scheduler.step()\n\n    train_loss = total_loss / total_samples\n    train_acc = total_correct / total_samples * 100\n    # Evaluate on test set\n    model.eval()\n    test_correct = 0\n    test_samples = 0\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            preds = outputs.argmax(dim=1)\n            test_correct += (preds == labels).sum().item()\n            test_samples += labels.size(0)\n    test_acc = test_correct / test_samples * 100\n\n    print(f\"Epoch {epoch:2d}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.2f}%, Test Acc={test_acc:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T14:46:54.610008Z","iopub.execute_input":"2025-10-04T14:46:54.610182Z","iopub.status.idle":"2025-10-04T16:31:23.549374Z","shell.execute_reply.started":"2025-10-04T14:46:54.610167Z","shell.execute_reply":"2025-10-04T16:31:23.548591Z"}},"outputs":[{"name":"stdout","text":"Epoch  1: Train Loss=2.0251, Train Acc=30.59%, Test Acc=44.67%\nEpoch  2: Train Loss=1.7463, Train Acc=42.23%, Test Acc=50.49%\nEpoch  3: Train Loss=1.6580, Train Acc=46.68%, Test Acc=52.65%\nEpoch  4: Train Loss=1.5890, Train Acc=49.58%, Test Acc=53.70%\nEpoch  5: Train Loss=1.5390, Train Acc=52.09%, Test Acc=57.77%\nEpoch  6: Train Loss=1.4963, Train Acc=54.20%, Test Acc=59.32%\nEpoch  7: Train Loss=1.4559, Train Acc=56.37%, Test Acc=58.30%\nEpoch  8: Train Loss=1.4231, Train Acc=58.03%, Test Acc=61.57%\nEpoch  9: Train Loss=1.3967, Train Acc=59.20%, Test Acc=62.68%\nEpoch 10: Train Loss=1.3710, Train Acc=60.32%, Test Acc=62.84%\nEpoch 11: Train Loss=1.3414, Train Acc=61.79%, Test Acc=65.36%\nEpoch 12: Train Loss=1.3102, Train Acc=63.57%, Test Acc=65.24%\nEpoch 13: Train Loss=1.2913, Train Acc=64.11%, Test Acc=66.72%\nEpoch 14: Train Loss=1.2629, Train Acc=65.76%, Test Acc=67.78%\nEpoch 15: Train Loss=1.2457, Train Acc=66.22%, Test Acc=69.49%\nEpoch 16: Train Loss=1.2240, Train Acc=67.18%, Test Acc=68.79%\nEpoch 17: Train Loss=1.1978, Train Acc=68.48%, Test Acc=69.12%\nEpoch 18: Train Loss=1.1850, Train Acc=69.29%, Test Acc=69.42%\nEpoch 19: Train Loss=1.1584, Train Acc=70.59%, Test Acc=70.58%\nEpoch 20: Train Loss=1.1397, Train Acc=71.18%, Test Acc=72.01%\nEpoch 21: Train Loss=1.1234, Train Acc=72.13%, Test Acc=71.61%\nEpoch 22: Train Loss=1.1019, Train Acc=73.18%, Test Acc=71.44%\nEpoch 23: Train Loss=1.0862, Train Acc=73.75%, Test Acc=73.41%\nEpoch 24: Train Loss=1.0597, Train Acc=74.98%, Test Acc=72.75%\nEpoch 25: Train Loss=1.0470, Train Acc=75.78%, Test Acc=74.43%\nEpoch 26: Train Loss=1.0252, Train Acc=76.71%, Test Acc=74.38%\nEpoch 27: Train Loss=1.0016, Train Acc=77.77%, Test Acc=73.53%\nEpoch 28: Train Loss=0.9809, Train Acc=78.78%, Test Acc=74.82%\nEpoch 29: Train Loss=0.9678, Train Acc=79.37%, Test Acc=75.24%\nEpoch 30: Train Loss=0.9456, Train Acc=80.50%, Test Acc=76.21%\nEpoch 31: Train Loss=0.9305, Train Acc=81.00%, Test Acc=75.94%\nEpoch 32: Train Loss=0.9048, Train Acc=82.13%, Test Acc=76.13%\nEpoch 33: Train Loss=0.8922, Train Acc=82.88%, Test Acc=76.51%\nEpoch 34: Train Loss=0.8749, Train Acc=83.66%, Test Acc=76.72%\nEpoch 35: Train Loss=0.8534, Train Acc=84.68%, Test Acc=76.45%\nEpoch 36: Train Loss=0.8407, Train Acc=85.27%, Test Acc=77.17%\nEpoch 37: Train Loss=0.8189, Train Acc=86.44%, Test Acc=76.42%\nEpoch 38: Train Loss=0.8046, Train Acc=87.16%, Test Acc=76.98%\nEpoch 39: Train Loss=0.7841, Train Acc=88.11%, Test Acc=77.14%\nEpoch 40: Train Loss=0.7687, Train Acc=88.91%, Test Acc=77.43%\nEpoch 41: Train Loss=0.7509, Train Acc=89.70%, Test Acc=76.91%\nEpoch 42: Train Loss=0.7394, Train Acc=90.33%, Test Acc=77.55%\nEpoch 43: Train Loss=0.7281, Train Acc=90.81%, Test Acc=76.64%\nEpoch 44: Train Loss=0.7183, Train Acc=91.27%, Test Acc=77.73%\nEpoch 45: Train Loss=0.7022, Train Acc=92.00%, Test Acc=77.30%\nEpoch 46: Train Loss=0.6914, Train Acc=92.51%, Test Acc=77.74%\nEpoch 47: Train Loss=0.6814, Train Acc=93.02%, Test Acc=77.99%\nEpoch 48: Train Loss=0.6728, Train Acc=93.40%, Test Acc=77.13%\nEpoch 49: Train Loss=0.6649, Train Acc=93.86%, Test Acc=77.92%\nEpoch 50: Train Loss=0.6520, Train Acc=94.47%, Test Acc=77.64%\nEpoch 51: Train Loss=0.6471, Train Acc=94.80%, Test Acc=77.53%\nEpoch 52: Train Loss=0.6372, Train Acc=95.17%, Test Acc=78.00%\nEpoch 53: Train Loss=0.6337, Train Acc=95.16%, Test Acc=78.03%\nEpoch 54: Train Loss=0.6242, Train Acc=95.76%, Test Acc=78.13%\nEpoch 55: Train Loss=0.6199, Train Acc=95.90%, Test Acc=77.85%\nEpoch 56: Train Loss=0.6123, Train Acc=96.27%, Test Acc=78.18%\nEpoch 57: Train Loss=0.6065, Train Acc=96.53%, Test Acc=78.21%\nEpoch 58: Train Loss=0.6056, Train Acc=96.43%, Test Acc=78.00%\nEpoch 59: Train Loss=0.6001, Train Acc=96.74%, Test Acc=78.49%\nEpoch 60: Train Loss=0.5933, Train Acc=97.06%, Test Acc=78.80%\nEpoch 61: Train Loss=0.5902, Train Acc=97.14%, Test Acc=78.95%\nEpoch 62: Train Loss=0.5858, Train Acc=97.36%, Test Acc=78.76%\nEpoch 63: Train Loss=0.5794, Train Acc=97.60%, Test Acc=78.76%\nEpoch 64: Train Loss=0.5790, Train Acc=97.64%, Test Acc=78.36%\nEpoch 65: Train Loss=0.5731, Train Acc=97.82%, Test Acc=78.21%\nEpoch 66: Train Loss=0.5690, Train Acc=97.95%, Test Acc=78.06%\nEpoch 67: Train Loss=0.5673, Train Acc=98.04%, Test Acc=78.76%\nEpoch 68: Train Loss=0.5666, Train Acc=98.08%, Test Acc=79.10%\nEpoch 69: Train Loss=0.5600, Train Acc=98.35%, Test Acc=79.32%\nEpoch 70: Train Loss=0.5582, Train Acc=98.43%, Test Acc=78.97%\nEpoch 71: Train Loss=0.5570, Train Acc=98.39%, Test Acc=78.83%\nEpoch 72: Train Loss=0.5541, Train Acc=98.53%, Test Acc=79.34%\nEpoch 73: Train Loss=0.5511, Train Acc=98.65%, Test Acc=78.88%\nEpoch 74: Train Loss=0.5495, Train Acc=98.72%, Test Acc=78.86%\nEpoch 75: Train Loss=0.5477, Train Acc=98.78%, Test Acc=79.06%\nEpoch 76: Train Loss=0.5466, Train Acc=98.76%, Test Acc=79.14%\nEpoch 77: Train Loss=0.5439, Train Acc=98.91%, Test Acc=79.05%\nEpoch 78: Train Loss=0.5451, Train Acc=98.85%, Test Acc=78.93%\nEpoch 79: Train Loss=0.5404, Train Acc=99.00%, Test Acc=79.42%\nEpoch 80: Train Loss=0.5404, Train Acc=98.97%, Test Acc=79.34%\nEpoch 81: Train Loss=0.5370, Train Acc=99.14%, Test Acc=79.06%\nEpoch 82: Train Loss=0.5371, Train Acc=99.09%, Test Acc=78.96%\nEpoch 83: Train Loss=0.5339, Train Acc=99.25%, Test Acc=79.38%\nEpoch 84: Train Loss=0.5350, Train Acc=99.16%, Test Acc=79.35%\nEpoch 85: Train Loss=0.5342, Train Acc=99.20%, Test Acc=79.23%\nEpoch 86: Train Loss=0.5325, Train Acc=99.31%, Test Acc=79.51%\nEpoch 87: Train Loss=0.5325, Train Acc=99.21%, Test Acc=79.52%\nEpoch 88: Train Loss=0.5323, Train Acc=99.27%, Test Acc=79.38%\nEpoch 89: Train Loss=0.5304, Train Acc=99.34%, Test Acc=79.42%\nEpoch 90: Train Loss=0.5307, Train Acc=99.33%, Test Acc=79.43%\nEpoch 91: Train Loss=0.5282, Train Acc=99.45%, Test Acc=79.44%\nEpoch 92: Train Loss=0.5289, Train Acc=99.43%, Test Acc=79.52%\nEpoch 93: Train Loss=0.5273, Train Acc=99.45%, Test Acc=79.45%\nEpoch 94: Train Loss=0.5293, Train Acc=99.40%, Test Acc=79.49%\nEpoch 95: Train Loss=0.5288, Train Acc=99.33%, Test Acc=79.58%\nEpoch 96: Train Loss=0.5280, Train Acc=99.39%, Test Acc=79.61%\nEpoch 97: Train Loss=0.5276, Train Acc=99.45%, Test Acc=79.55%\nEpoch 98: Train Loss=0.5285, Train Acc=99.35%, Test Acc=79.57%\nEpoch 99: Train Loss=0.5273, Train Acc=99.43%, Test Acc=79.57%\nEpoch 100: Train Loss=0.5274, Train Acc=99.47%, Test Acc=79.55%\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"print(test_acc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:34:45.484191Z","iopub.execute_input":"2025-10-04T16:34:45.484888Z","iopub.status.idle":"2025-10-04T16:34:45.489840Z","shell.execute_reply.started":"2025-10-04T16:34:45.484852Z","shell.execute_reply":"2025-10-04T16:34:45.489070Z"}},"outputs":[{"name":"stdout","text":"79.55\n","output_type":"stream"}],"execution_count":4}]}